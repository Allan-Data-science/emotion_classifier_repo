# -*- coding: utf-8 -*-
"""emotion_classifier/model_loader

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17lAaDskGGC6UrQVfeAp7vm8pjCphZHN2
"""

# emotion_classifier/model_loader.py
import os
import requests
import joblib
import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from pathlib import Path
import nltk # For NLTK data path configuration

# --- Configuration (Adjust these as needed) ---
# This should match the model you trained (e.g., 'roberta-base')
TRAINED_MODEL_NAME_OR_PATH = 'roberta-base'
NUM_LABELS = 6 # Number of emotion classes your model was trained on

# Directory to store downloaded assets within the package
# This path is relative to where model_loader.py is located
ASSETS_DIR = Path(__file__).parent / 'assets'
ASSETS_DIR.mkdir(parents=True, exist_ok=True) # Ensure directory exists

# --- IMPORTANT: Replace these with your actual direct download links ---
# Example: Google Drive direct download link (see notes below on how to get this)
MODEL_STATE_DICT_URL = "YOUR_DIRECT_DOWNLOAD_LINK_FOR_MODEL_STATE_DICT.pt"
LABEL_ENCODER_URL = "YOUR_DIRECT_DOWNLOAD_LINK_FOR_LABEL_ENCODER.pkl"
# Tokenizer files are usually small enough or can be re-downloaded by Hugging Face,
# but if you have custom tokenizer files, provide their links too.

MODEL_STATE_DICT_PATH = ASSETS_DIR / 'best_transformer_model_state_macro_f1.pt'
LABEL_ENCODER_PATH = ASSETS_DIR / 'label_encoder.pkl'

# NLTK data path configuration for the package
NLTK_PACKAGE_DATA_PATH = ASSETS_DIR / 'nltk_data'
if not NLTK_PACKAGE_DATA_PATH.exists():
    NLTK_PACKAGE_DATA_PATH.mkdir(parents=True, exist_ok=True)
if str(NLTK_PACKAGE_DATA_PATH) not in nltk.data.path:
    nltk.data.path.append(str(NLTK_PACKAGE_DATA_PATH))

def download_nltk_resource_for_package(resource_id, resource_name_for_find):
    try:
        nltk.data.find(resource_name_for_find, paths=[str(NLTK_PACKAGE_DATA_PATH)])
        print(f"NLTK resource '{resource_id}' found in package assets.")
    except LookupError:
        print(f"NLTK resource '{resource_id}' not found in package assets. Downloading...")
        try:
            nltk.download(resource_id, quiet=False, download_dir=str(NLTK_PACKAGE_DATA_PATH))
            print(f"NLTK resource '{resource_id}' downloaded to package assets.")
        except Exception as e:
            print(f"Failed to download NLTK resource '{resource_id}': {e}")
            print("Please ensure NLTK resources (stopwords, wordnet, omw-1.4) are available or can be downloaded.")


def download_file(url, destination_path):
    """Downloads a file from a URL to a destination path."""
    if "YOUR_DIRECT_DOWNLOAD_LINK" in url: # Check if placeholder URL is still there
        print(f"Placeholder download URL for {destination_path.name}. Skipping download.")
        print(f"Please replace '{url}' in model_loader.py with your actual direct download link.")
        return False # Indicate download was skipped

    print(f"Downloading {destination_path.name} from {url}...")
    try:
        response = requests.get(url, stream=True)
        response.raise_for_status()  # Raise an exception for HTTP errors
        with open(destination_path, "wb") as f:
            for chunk in response.iter_content(chunk_size=8192):
                f.write(chunk)
        print(f"{destination_path.name} downloaded successfully to {destination_path}")
        return True
    except requests.exceptions.RequestException as e:
        print(f"Error downloading {destination_path.name}: {e}")
        return False

def get_model_assets():
    """
    Ensures model assets (state_dict, label_encoder) are available locally,
    downloading them if necessary. Loads and returns them.
    """
    # Ensure NLTK resources are available for preprocessing
    download_nltk_resource_for_package('stopwords', 'corpora/stopwords')
    download_nltk_resource_for_package('wordnet', 'corpora/wordnet')
    download_nltk_resource_for_package('omw-1.4', 'corpora/omw-1.4')


    # Download model state dict if not present
    if not MODEL_STATE_DICT_PATH.exists():
        if not download_file(MODEL_STATE_DICT_URL, MODEL_STATE_DICT_PATH):
            raise FileNotFoundError(f"Model state dict not found at {MODEL_STATE_DICT_PATH} and download failed/skipped. Please check URL and internet connection.")

    # Download label encoder if not present
    if not LABEL_ENCODER_PATH.exists():
        if not download_file(LABEL_ENCODER_URL, LABEL_ENCODER_PATH):
            raise FileNotFoundError(f"Label encoder not found at {LABEL_ENCODER_PATH} and download failed/skipped. Please check URL and internet connection.")

    # Load assets
    print(f"Loading model from {TRAINED_MODEL_NAME_OR_PATH} and weights from {MODEL_STATE_DICT_PATH}...")
    model = AutoModelForSequenceClassification.from_pretrained(
        TRAINED_MODEL_NAME_OR_PATH,
        num_labels=NUM_LABELS,
        # No need for ignore_mismatched_sizes=True if loading a state_dict into this structure
    )
    model.load_state_dict(torch.load(MODEL_STATE_DICT_PATH, map_location=torch.device('cpu')))
    model.eval() # Set to evaluation mode

    print(f"Loading tokenizer for {TRAINED_MODEL_NAME_OR_PATH}...")
    tokenizer = AutoTokenizer.from_pretrained(TRAINED_MODEL_NAME_OR_PATH)

    print(f"Loading label encoder from {LABEL_ENCODER_PATH}...")
    label_encoder = joblib.load(LABEL_ENCODER_PATH)

    return model, tokenizer, label_encoder